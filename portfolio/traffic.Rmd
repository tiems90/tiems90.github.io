---
title: "Madrid - Real-time Traffic"
author: "Timo"
date: 4/15/2017
image: 'img/portfolio/mad.png'
output:
  blogdown::html_page:
    toc: true
    number_sections: true
    df_print: paged
description: "This workbook is mostly concerned with cleaning and analysing both real-time and historic traffic data in Madrid." 
---

#Load Data
Let's start with some open, real-time, Madrid traffic data!
First we load up our packages and download our data. Our real-time data can be downloaded through the link in our code on `datos.madrid.es`. The measured points are a bit further hidden away on the website. But can be accessed [here](http://datos.madrid.es/portal/site/egob/menuitem.c05c1f754a33a9fbe4b2e4b284f1a5a0/?vgnextoid=ee941ce6ba6d3410VgnVCM1000000b205a0aRCRD&vgnextchannel=374512b9ace9f310VgnVCM100000171f5a0aRCRD), and downloaded as a zip file. 

```{r loading data, results='hide',message=FALSE, warning=FALSE}
#Data Loading
library(XML)
library(sp)
library(readr)
library(data.table)
library(spacetime)

#Data Wrangling & Analysis
library(tidyverse)
library(reshape2)
library(outliers)
library(corrplot)
library(lubridate)
library(dtplyr)

#Maps
library(rgdal)
library(tmap)
library(tmaptools)

#Load Data
traffic0 <- xmlToDataFrame('http://datos.madrid.es/egob/catalogo/202087-0-trafico-intensidad.xml') 
head(traffic0)
measured.points <- readOGR(dsn = path.expand("~/Documents/R/Git/data/data_madrid"), 'pmed_trafico')

#Set Parameters
proj <- '+proj=utm +zone=30 +ellps=intl +towgs84=-87,-98,-121,0,0,0,0 +units=m +no_defs'
pal <- get_brewer_pal('YlOrRd', 5, plot = FALSE)
```

#Data Cleaning
##Georreferencing
Now that we have loaded our packages and data, we find something interesting. The shapefile we loaded lacks a projection. Unfortunately the documentation does not specify the projection we need. After much googling, we wind up at the [Guía básica para el manejo de archivos Shapefile](http://datos.madrid.es/FWProjects/egob/contenidos/datasets/ficheros/Manejo%20básico%20de%20archivos%20shp.pdf) which allows us to set our proj4 string as the following:

`r proj`


```{r mapping data}
proj4string(measured.points) <- proj

madrid <- read_osm(measured.points, type = 'maptoolkit-topo')
tm_shape(madrid)+
  tm_raster()+
  tm_shape(measured.points)+
  tm_dots(col = 'blue')+
  tm_scale_bar()+
  tm_style_natural() +
  tm_layout(title = 'Measured Points')
```

##Data Cleaning
Some weird things are still in our data. These will need to be looked at before we do anything else. 
A few histograms already show us that some of the data is encoded as negative. They do not provide an explanation of this in the data, but it seems to occurr when we receive an error message. Therefore we will remove the sensors providing an error message. 

Let's start by renaming the columns so they make more sense and see what it looks like after we do that. 
```{r}
traffic <- traffic0 %>% 
  mutate(
    cars_hour = as.numeric(intensidad),
    control_pnt_full = as.numeric(ocupacion),
    load = as.numeric(carga),
    saturation = as.numeric(intensidadSat),
    avg_speed = as.numeric(velocidad)
  ) %>% 
  select(codigo, descripcion, cars_hour, control_pnt_full, load, saturation, avg_speed, error)

#Quick look at the data
head(traffic)
```

Here I spotted something strange. Cars/Hour takes a negative value at times. Let's explore this a bit further in a plot. 

```{r, echo=FALSE}
#Values below 0
hist(traffic$cars_hour, right = F, main = 'Values Below Zero')
```

It looks like we have quite a few of these. After looking around in the data it seems these errors occur in the instances that there is an error in the machine. So let's remove all the sensors who are giving error messages. 

``` {r}
#Remove non-functioning sensors. 
traffic <- traffic[!traffic$error=='0 0 0 0 0 0 0 0' &
                   !traffic$error=='S',]
```

And let's see if that fixed our problem!
```{r, echo=FALSE}
#After accounting for the erros in sensors, we no longer find any. 
hist(traffic$cars_hour, right = F, main = 'Values Below Zero')
```

The documentation also metions that in our measuring points data, there is a factor that indicates urban or interurban positions. Let's update these factor labels. 

```{r}
#current labels
levels(measured.points@data$tipo_elem)
#Let's update these. 
levels(measured.points@data$tipo_elem) <- c('Highway', 'City Roads')
```



##Merging
Now that we have our plotted data and the live data from Madrid, we can merge the two together into one SP dataframe.
The documentation, `PUNTOS MEDIDA TRAFICO_MADRID.pdf`, indicates that in our `traffic` the column `codigo` should coincide with `COD_CENT` in our georeferenced set. We can use this as a key to merge the two. 

```{r}
traffic.full <- merge(measured.points, traffic, 
                      by.x='cod_cent', by.y='codigo', 
                      duplicateGeoms = TRUE, all.x=F)
head(traffic.full@data)
```

#Exploratory Data Analysis
Let's have a look at our data now. Just to get a better idea of what we're working with. 
To make things easer on ourselves later, I only consider the cleaned data that merges well. 

##Missing Data
Let's see how many missing values we have. 
```{r}
df <- as.tbl(traffic.full@data)
sapply(df, function(y) sum(is.na(y)))
```
So we are missing descriptions, which shouldn't be too bad, since the name should be captured. We're missing a lot of values for speed, speed and saturation/description. We will later see that this is because only the highways have spedometers, and the saturation levels are only calculated for city roads. It is not too troubling, except for later when we look at correlations. We'll keep it in mind for the later excercises. 

##Distributions
Let's see how our data are distributed. 
```{r warning=FALSE}
df %>% 
  select(1, 7:11) %>% 
  melt("cod_cent") %>% 
  ggplot(aes(x=value))+
  geom_density(fill='grey')+
  facet_wrap(~variable, scales = 'free')
```

Key things to note here are the crazy outliers we have in cars per hour for some areas, and our other data. 
Perhaps these are a function of the road type? Let's have a look. 

```{r warning=FALSE}
df %>% 
  select(1,3, 7:11) %>% 
  melt(c("cod_cent", 'tipo_elem')) %>% 
  ggplot(aes(x=value,  group=tipo_elem, fill=tipo_elem))+
  geom_density(alpha=0.8)+
  facet_wrap(~variable, scales = 'free')
```

A first thing to note here is that the speed data is only available on the highway, whereas we find the saturation levels only on city roads. A quick check with our source documentation confirms this to be the case. 
However, It turns out it's a bit hard to see this due to all the outliers tampering with our tails in the distribution. Let's get rid of these first, and then recreate the graphs from above. To do this quickly, we use the `outliers` package. 

```{r}
df2 <- df
df2[,7:11] <- df[,7:11] %>% 
   rm.outlier(fill=TRUE, median=TRUE)

df2 %>% 
  select(1,3, 7:11) %>% 
  melt(c("cod_cent", 'tipo_elem')) %>% 
  ggplot(aes(x=value,  group=tipo_elem, fill=tipo_elem))+
  geom_density(alpha=0.8)+
  facet_wrap(~variable, scales = 'free')
```

There we go. A lot more informative. One key thing to notice immediately is that our city roads have much fewer cars per hour than our highways: which makes a lot of sense. We also see that they aren't used quite as heavily as the highway. 

I think we've done about all we can with the distributions. Let's have a look at the correlations in our data. 

##Correlations
Let's have a quick look at the correlations of the variables. 
Since two variables overlap, we'll make two charts. 
```{r}
#City Roads
df %>% 
  filter(tipo_elem == 'City Roads') %>% 
  select(7:10) %>% 
  cor(use='complete') %>% 
  corrplot('square')

#Highway
df %>% 
  filter(tipo_elem == 'Highway') %>% 
  select(7:11, -10) %>% 
  cor(use='complete') %>% 
  corrplot('square')
```

Our main conclusions here:

* Saturation and load are positively correlated with cars per hour. This makes sense, as these all measure how busy a given road is. 
* On highways, number of cars by the control point are also positively correlated with these. So is the average speed. This would suggest that as there's more traffic passing per hour, the average speed tends to be higher. (Although I'm sure that during traffic jams, this would look differently.)
* On city roads, it all tends to be a bit less strongly correlated. The exception being the load and cars at the control points. Strangly, Saturation appears to be slightly negatively correlated with load and control points being full. This is against expactations. 

#Maps
Now that we have joined, cleaned, and briefly analysed our data we can start looking at more interesting plots.
Let's start by finding out speedy roads. 

##Speed

We subset our dataset to the points we have speed data on and recreate our map. 
```{r}
speed <- subset(traffic.full, !is.na(avg_speed))
head(speed@data)

#Download a simpler map so it's easier to see for colourblind me. 
madrid.spd <- read_osm(speed, type = 'stamen-toner')

#Going to reverse the pallete here, so red can mean congested/slow
tm_shape(madrid.spd)+
  tm_raster()+
  tm_shape(speed)+
  tm_dots(col = 'avg_speed', palette= rev(pal),size = 0.2) + 
  tm_scale_bar() +
  tm_style_natural()
```

##Congestion
Let's have a similar look at the most congested roads!

```{r}
saturation <- subset(traffic.full, !is.na(saturation))
head(saturation@data)

#Download a simpler map so it's easier to see for colourblind me. 
madrid.sat <- read_osm(saturation, type = 'stamen-toner')

tm_shape(madrid.sat)+
  tm_raster()+
  tm_shape(saturation)+
  tm_dots(col = 'saturation', palette=pal, size = 0.1) +
  tm_scale_bar()+
  tm_style_natural()
```

#Extension

Now let's timeset this data. The [documentation](http://datos.madrid.es/FWProjects/egob/contenidos/datasets/ficheros/Estructura%20y%20contenido%20del%20fichero%20csv.pdf) specifies how to treat this data. First of all, since we'll be working with a dataset that is about 700 MB large, I'll be loading up the data.table package and working with that so we can use `fread()`. 

##Load the data
Now I would like to extend this with historic data. To save my computer I will only use one month's worth of data, February 2017. `Actually, this is hurting my Laptop substantially, so I will be looking at my birthday only.`

```{r Load Big Dataset}
#Load data
traffic_feb <- fread("~/Documents/R/Madrid - Traffic/02-2017.csv", sep = ';')

#Quick Peak
glimpse(traffic_feb)

#Recode our time data. 
traffic_feb$fecha <- ymd_hms(traffic_feb$fecha)

bday <- traffic_feb %>% 
  filter(day(fecha)==17)

rm(traffic_feb)
```

##Quick Visual Analysis
Now, the added benefit of having this data as a time series is that we can look at figures over time. Let's look at traffic data over time. 

```{r}
bday %>%
  select(fecha, tipo_elem, identif, vmed) %>% 
  filter(vmed > 0) %>% 
  ggplot(aes(fecha, vmed, group=identif)) +
  geom_line(alpha=0.09)
```

Main conclusion number one: it's difficult to draw conclusions when there's this many observations. We do, however, clearly see when it's rush hour. Does this make sense? It was on a `r weekdays(dmy(17022017))`. So Yes! That also explains why we see people leaving work from around 14:00. 

Let's delve into this a bit more by aggregating our data to city roads and the highway. 

```{r warning=FALSE}


bday %>% 
  group_by(fecha, tipo_elem) %>% 
  summarise(
    cars_hour = mean(intensidad),
    road_use = mean(carga),
    avg_speed = mean(vmed),
    errors = sum(error!='N') #It appears there's no faulty machinery on this day. Hurrah!
  ) %>% 
  melt(c('fecha', 'tipo_elem')) %>% 
  ggplot(aes(x=fecha, y=value, group=tipo_elem, col=tipo_elem)) +
  geom_line() +
  facet_grid(variable~., scales = 'free_y')
```

As such it seems to be that when we average things out, there are similar patterns to be found here as in the real-time data. There are fewer overall cars per hour, put road usage is very similar, if not higher in urban settings. 

Just as a quick curiosity: I wonder what's the highest average speed recorded this day. `r max(bday$vmed)` KmH. Not bad!

##Plotting Average Speed over Time
`Warning: This section involves some heavy data cleaning`

Now, after reading the documentation and seeing the `animation_tmap()` function, this sounds like something interesting to do! In order to keep my computer alive I will average by the hour though. Sadly, thusfar I haven't been able to get any animations going, and in order to accomodate panel data, I was not able to use the `tmap` package, but had to switch to `spacetime`. The results are seen below. (Also, yes. I needed to find the most common occuring element, so I had to create a mode function.)

```{r}
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

bday.hr <- bday %>% 
  mutate(hour = floor_date((fecha), 'hour')) %>%
  group_by(idelem, hour) %>% 
  summarise(
    cars_hour = mean(intensidad),
    road_use = mean(carga),
    avg_speed = mean(vmed),
    type = Mode(tipo_elem)
  )

#First we need to subset our SP object.
measured.points.red <- subset(measured.points, idelem %in% bday.hr$idelem)

#It's not NA data. 
sapply(bday.hr, function(y) sum(is.na(y)))

ids <- unique(bday.hr$idelem)
id.occurance <- 0
for (i in 1:length(ids)){
  id.occurance[i] <- sum(bday.hr$idelem==ids[i])
}

id.df <- data.frame(ID= ids, N=id.occurance)
id.df %>% 
  ggplot(aes(ID, N)) +
  geom_histogram(stat='identity')

#So it seems like we have quite a few missing. Let's clean this out.
#Most have an occurance of:
Mode(id.df$N)
#Let's remove all those that aren't at this level. 
remove.id <- id.df %>% filter(N!= Mode(id.df$N))
measured.points.red <- subset(measured.points.red, !(idelem %in% remove.id$ID))
bday.hr.red <- bday.hr %>%  filter(!(idelem %in% remove.id$ID))

#Let's look at our key variable. It looks like our sp data is coded as factors. 
str(measured.points.red@data)
str(bday.hr.red)

#Shame on me for not cleaning this properly to begin with. 
measured.points.red@data$idelem <- measured.points.red@data$idelem %>% 
  as.character %>% 
  as.numeric

#Let's just all mash it into one DF then. 
df.coord <- data.frame(idelem = measured.points.red$idelem, x = coordinates(measured.points.red)[,1], 
                  y = coordinates(measured.points.red)[,2])

df.bday <- merge(bday.hr.red, df.coord, by='idelem', all.x=T)
df.bday <- df.bday[complete.cases(df.bday),]

bday.hr.stidf <- stConstruct(df.bday, c("x", "y"), "hour", interval = TRUE, crs = CRS(proj))
class(bday.hr.stidf) #success!

#Speedplot
speed.stidf <- subset(bday.hr.stidf, bday.hr.stidf@data$avg_speed>0)
stplot(speed.stidf[,,'avg_speed'], number=24, alpha=0.7, cex=0.4)

```

##Road use over time

Now that the hard work is over, we can simply call the functions and dataframes from the previous section. Since nighttime isn't as interesting to us, we will filter it. 

```{r}
#Road Use
road_use.stidf <- subset(bday.hr.stidf, hour(bday.hr.stidf@time)>6)
stplot(road_use.stidf[,,'road_use'], alpha=1, cex=0.25, number=17)
```

##Cars per Hour
Now, for our final plot we can look at the cars per hour metric. It makes sense to split this into one for the highway and one for the inner city, so let's do this. 

```{r}
cars_hour.city.stidf <- subset(bday.hr.stidf, 
                               bday.hr.stidf@data$type=='PUNTOS MEDIDA URBANOS')
cars_hour.highway.stidf <- subset(bday.hr.stidf, 
                               bday.hr.stidf@data$type!='PUNTOS MEDIDA URBANOS')

#This plot comes out nicely.
stplot(cars_hour.highway.stidf[,,'cars_hour'], alpha=1, cex=0.25, number=24)

#However, for the city one we will have to take a log to scale it. 
cars_hour.city.stidf@data$cars_hour <- (cars_hour.city.stidf@data$cars_hour+1) %>% log
stplot(cars_hour.city.stidf[,,'cars_hour'], alpha=1, cex=0.25, number=24)
```

And that's all for now! :-) 